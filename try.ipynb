{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import struct\n",
    "from array import array\n",
    "from typing import List, Literal\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import clone\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset: Literal['covertype', 'glass', 'mnist', 'skin', 'shuttle', 'usps', 'wine', 'yeast']):\n",
    "    if dataset == 'mnist':\n",
    "        def read_images_labels(images_filepath, labels_filepath):        \n",
    "            labels = []\n",
    "            with open(os.path.join(os.path.dirname(__file__), 'datasets', 'mnist', labels_filepath), 'rb') as file:\n",
    "                magic, size = struct.unpack(\">II\", file.read(8))\n",
    "                if magic != 2049:\n",
    "                    raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "                labels = array(\"B\", file.read())        \n",
    "            \n",
    "            with open(os.path.join(os.path.dirname(__file__), 'datasets', 'mnist', images_filepath), 'rb') as file:\n",
    "                magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "                if magic != 2051:\n",
    "                    raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "                image_data = array(\"B\", file.read())        \n",
    "            images = []\n",
    "            for i in range(size):\n",
    "                images.append([0] * rows * cols)\n",
    "            for i in range(size):\n",
    "                img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "                img = img.reshape(28, 28)\n",
    "                images[i][:] = img            \n",
    "            \n",
    "            return np.array(images), np.array(labels)\n",
    "                \n",
    "        def load_data():\n",
    "            x_train, y_train = read_images_labels('train-images.idx3-ubyte', 'train-labels.idx1-ubyte')\n",
    "            x_test, y_test = read_images_labels('t10k-images.idx3-ubyte', 't10k-labels.idx1-ubyte')\n",
    "            return (x_train, y_train),(x_test, y_test)   \n",
    "\n",
    "\n",
    "        (X_train, y_train),(X_test, y_test) = load_data()\n",
    "        X_train = X_train.reshape(-1, 28 * 28)\n",
    "        X_test = X_test.reshape(-1, 28 * 28)\n",
    "        \n",
    "        X = np.vstack((X_train, X_test))\n",
    "        y = np.hstack((y_train, y_test))\n",
    "        \n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y)\n",
    "    elif dataset == 'usps': \n",
    "        with h5py.File(os.path.join('datasets', 'usps', 'usps.h5'), 'r') as hf:\n",
    "                train = hf.get('train')\n",
    "                X_train = train.get('data')[:]\n",
    "                y_train = train.get('target')[:]\n",
    "                test = hf.get('test')\n",
    "                X_test = test.get('data')[:]\n",
    "                y_test = test.get('target')[:]\n",
    "        \n",
    "        X = np.vstack((X_train, X_test))\n",
    "        y = np.hstack((y_train, y_test))\n",
    "        \n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y)\n",
    "    elif dataset == 'wine':\n",
    "        df = pd.read_csv(os.path.join('datasets', 'wine', 'winequality-white.csv'), delimiter=';')\n",
    "        X = df.iloc[:, :-1]\n",
    "        y = df.iloc[:, -1]\n",
    "\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y)\n",
    "\n",
    "        # X_train = X_train.to_numpy(dtype=float)\n",
    "        # X_test = X_test.to_numpy(dtype=float)\n",
    "    elif dataset == 'yeast':\n",
    "        df = pd.read_csv(os.path.join('datasets', 'yeast', 'yeast.data'), sep='\\\\s+', header=None)\n",
    "        X = df.iloc[:, 1:-1]\n",
    "        y = df.iloc[:, -1]\n",
    "\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y)\n",
    "\n",
    "        # X_train = X_train.to_numpy(dtype=float)\n",
    "        # X_test = X_test.to_numpy(dtype=float)\n",
    "    elif dataset == 'glass':\n",
    "        df = pd.read_csv(os.path.join('datasets', 'glass', 'glass.data'), sep=',', header=None)\n",
    "        X = df.iloc[:, 1:-1]\n",
    "        y = df.iloc[:, -1]\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y)\n",
    "\n",
    "        # X_train = X_train.to_numpy(dtype=float)\n",
    "        # X_test = X_test.to_numpy(dtype=float)\n",
    "    elif dataset == 'covertype':\n",
    "        df = pd.read_csv(os.path.join('datasets', 'covertype', 'covtype.data'), header=None)\n",
    "        X = df.iloc[:, :-1]\n",
    "        y = df.iloc[:, -1]\n",
    "\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y)\n",
    "\n",
    "\n",
    "        # X_train = X_train.to_numpy(dtype=int)\n",
    "        # X_test = X_test.to_numpy(dtype=int)\n",
    "    elif dataset == 'skin':\n",
    "        df = pd.read_csv(os.path.join('datasets', 'skin_nonskin', 'Skin_NonSkin.txt'), sep='\\\\s+', header=None)\n",
    "        X = df.iloc[:, :-1]\n",
    "        y = df.iloc[:, -1]\n",
    "\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y)\n",
    "\n",
    "        # X_train = X_train.to_numpy(dtype=int)\n",
    "        # X_test = X_test.to_numpy(dtype=int)\n",
    "    elif dataset == 'statlog':\n",
    "        df_train = pd.read_csv(os.path.join('datasets', 'statlog', 'shuttle.trn'), sep='\\\\s+', header=None)\n",
    "        df_test = pd.read_csv(os.path.join('datasets', 'statlog', 'shuttle.trn'), sep='\\\\s+', header=None)\n",
    "        X_train = df_train.iloc[:, :-1]\n",
    "        y_train = df_train.iloc[:, -1]\n",
    "        X_test = df_test.iloc[:, :-1]\n",
    "        y_test = df_test.iloc[:, -1]\n",
    "\n",
    "        y_train = LabelEncoder().fit_transform(y_train)\n",
    "        y_test = LabelEncoder().fit_transform(y_test)\n",
    "\n",
    "        X_train = X_train.to_numpy(dtype=int)\n",
    "        X_test = X_test.to_numpy(dtype=int)\n",
    "\n",
    "        X = np.vstack((X_train, X_test))\n",
    "        y = np.hstack((y_train, y_test))\n",
    "        \n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y)\n",
    "    else:\n",
    "        raise Exception(f'unknown dataset {dataset}')\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results(dataset: str, ks: List[int], thresholds: List[float], knn_algo:Literal['brute', 'kd_tree', 'ball_tree']='brute'):\n",
    "    logger.info(f'Generating results for dataset: {dataset}')\n",
    "    save_folder = os.path.join('results', dataset)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    \n",
    "    X, y = load_dataset(dataset)\n",
    "    from collections import Counter\n",
    "\n",
    "    class_counts = Counter(y)\n",
    "\n",
    "    # # Find the class with the least members\n",
    "    # least_common_class, least_common_count = min(class_counts.items(), key=lambda item: item[1])\n",
    "    # print(f\"Class with the least members: {least_common_class}, Number of members: {least_common_count}\")\n",
    "    print(\"Class distribution before removal:\")\n",
    "    class_counts = Counter(y)\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"Class {cls}: {count} members\")\n",
    "\n",
    "\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "    i=0\n",
    "    for train_index, test_index in rskf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        if not str in ('statlog', 'mnist', 'usps'):\n",
    "            X_train = X_train.to_numpy(dtype=int)\n",
    "            X_test = X_test.to_numpy(dtype=int)\n",
    "        \n",
    "\n",
    "        def smart_decision(clf, sample, neighbor_idxs):\n",
    "            X_neighbors, y_neighbors = X_train[neighbor_idxs], y_train[neighbor_idxs]\n",
    "\n",
    "            unique, counts = np.unique(y_neighbors, return_counts=True)\n",
    "            dominant_class = unique[np.argmax(counts)]\n",
    "            if counts[np.argmax(counts)] >= treshold*k:\n",
    "                return dominant_class\n",
    "            \n",
    "            else:\n",
    "                clf.fit(X_neighbors, y_neighbors)\n",
    "\n",
    "                return clf.predict(sample.reshape(1, -1))[0]\n",
    "            \n",
    "        def pipeline_name(clf):\n",
    "            if clf.__class__.__name__ == \"Pipeline\":\n",
    "                return clf[-1].__class__.__name__\n",
    "            else:\n",
    "                return clf.__class__.__name__\n",
    "        \n",
    "        logistic_clf = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "        svm_clf = make_pipeline(StandardScaler(), SVC())\n",
    "\n",
    "        clfs = [svm_clf, GaussianNB(), logistic_clf, DecisionTreeClassifier()]\n",
    "        clfs = tuple(sorted(clfs, key=lambda clf: pipeline_name(clf)))\n",
    "        \n",
    "        baseline_knn_acc = np.empty((len(ks)))\n",
    "        baseline_knn_time = np.empty((len(ks)))\n",
    "        \n",
    "        baseline_acc = np.empty((len(clfs)))\n",
    "        baseline_time = np.empty((len(clfs)))\n",
    "        \n",
    "        smart_acc = np.empty((len(clfs), len(ks), len(thresholds)))\n",
    "        smart_time = np.empty((len(clfs), len(ks), len(thresholds)))\n",
    "\n",
    "        for iclf, clf in enumerate(clfs):\n",
    "            logger.info(f\"calcing (baseline) for clf: {pipeline_name(clf)}\")\n",
    "            start = timer()\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred_test_rf = clf.predict(X_test)\n",
    "            end = timer()\n",
    "            \n",
    "            baseline_time[iclf]=end-start\n",
    "            baseline_acc[iclf] = accuracy_score(y_test, y_pred_test_rf)\n",
    "        \n",
    "        for ik, k in enumerate(ks):\n",
    "            logger.info(f\"\\tcalcing for k: {k}\")\n",
    "\n",
    "            knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute', metric='minkowski', p=2, n_jobs=-1)\n",
    "            \n",
    "            start = timer()\n",
    "            \n",
    "            knn.fit(X_train, y_train)\n",
    "            y_pred_test_knn = knn.predict(X_test)\n",
    "            \n",
    "            end = timer()\n",
    "            \n",
    "            baseline_knn_time[ik] = end - start\n",
    "            baseline_knn_acc[ik] = accuracy_score(y_test, y_pred_test_knn)\n",
    "\n",
    "            neighbors_test = knn.kneighbors(X_test, return_distance=False)   \n",
    "\n",
    "            for iclf, clf in enumerate(clfs):\n",
    "                logger.info(f\"\\t\\tcalcing (smart) for clf: {pipeline_name(clf)}\")\n",
    "                for itreshold, treshold in enumerate(thresholds):\n",
    "                    logger.info(f\"\\t\\t\\tcalcing (smart) for threshold: {treshold}\")\n",
    "                    start = timer()\n",
    "                    y_pred_test_smart=Parallel(n_jobs=-1)(delayed(smart_decision)(clone(clf), X_test[i], idxs) for i, idxs in enumerate(neighbors_test))\n",
    "                    end = timer()\n",
    "\n",
    "                    smart_time[iclf, ik, itreshold]=end-start\n",
    "                    smart_acc[iclf, ik, itreshold]=accuracy_score(y_test, y_pred_test_smart)\n",
    "        logger.info(f'~~Finished~~ Generating results for dataset: {dataset}')\n",
    "        results = {\n",
    "            \"baseline_knn_time\": baseline_knn_time,\n",
    "            \"baseline_knn_acc\": baseline_knn_acc,\n",
    "            \"baseline_time\": baseline_time,\n",
    "            \"baseline_acc\": baseline_acc,\n",
    "            \"smart_time\": smart_time,\n",
    "            \"smart_acc\": smart_acc,\n",
    "            \"clfs\": [pipeline_name(clf) for clf in clfs],\n",
    "            \"ks\": ks,\n",
    "            \"tresholds\": thresholds,\n",
    "            \"knn_algo\":knn_algo\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(save_folder, f'results_{i}_{knn_algo}.pickle'), 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ks = [2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 50, 80, 100]\n",
    "    thresholds = [1]\n",
    "    datasets1 = ['covertype', 'glass', 'mnist', 'skin', 'shuttle', 'usps', 'wine', 'yeast']\n",
    "    datasets = ['glass']\n",
    "    knn_algo: Literal['brute', 'kd_tree', 'ball_tree'] = 'brute'\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        generate_results(dataset, ks=ks, thresholds=thresholds, knn_algo=knn_algo)\n",
    "    #hi daniel"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
